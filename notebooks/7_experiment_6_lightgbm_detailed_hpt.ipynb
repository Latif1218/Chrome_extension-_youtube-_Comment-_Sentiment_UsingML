{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f2f79c1-c027-47bb-9d0e-82c036fb2994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlflow in c:\\jupyter\\venv\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: boto3 in c:\\jupyter\\venv\\lib\\site-packages (1.39.4)\n",
      "Requirement already satisfied: awscli in c:\\jupyter\\venv\\lib\\site-packages (1.41.4)\n",
      "Requirement already satisfied: optuna in c:\\jupyter\\venv\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\jupyter\\venv\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: lightgbm in c:\\jupyter\\venv\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: mlflow-skinny==3.1.1 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow) (3.1.1)\n",
      "Requirement already satisfied: Flask<4 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow) (3.1.1)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow) (1.16.4)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow) (3.4.3)\n",
      "Requirement already satisfied: matplotlib<4 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<3 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow) (2.3.1)\n",
      "Requirement already satisfied: pandas<3 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow) (2.3.1)\n",
      "Requirement already satisfied: pyarrow<21,>=4.0.0 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow) (20.0.0)\n",
      "Requirement already satisfied: scikit-learn<2 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow) (1.6.1)\n",
      "Requirement already satisfied: scipy<2 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow) (1.15.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow) (2.0.38)\n",
      "Requirement already satisfied: waitress<4 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow) (3.0.2)\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (8.1.8)\n",
      "Requirement already satisfied: cloudpickle<4 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.1)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (0.58.0)\n",
      "Requirement already satisfied: fastapi<1 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (0.116.1)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.44)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (8.7.0)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (1.35.0)\n",
      "Requirement already satisfied: packaging<26 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (24.2)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (6.31.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.10.8 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (2.11.7)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (2.32.3)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (4.12.2)\n",
      "Requirement already satisfied: uvicorn<1 in c:\\jupyter\\venv\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (0.35.0)\n",
      "Requirement already satisfied: Mako in c:\\jupyter\\venv\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
      "Requirement already satisfied: colorama in c:\\jupyter\\venv\\lib\\site-packages (from click<9,>=7.0->mlflow-skinny==3.1.1->mlflow) (0.4.6)\n",
      "Requirement already satisfied: google-auth~=2.0 in c:\\jupyter\\venv\\lib\\site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (2.40.3)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\jupyter\\venv\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (308)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\jupyter\\venv\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in c:\\jupyter\\venv\\lib\\site-packages (from fastapi<1->mlflow-skinny==3.1.1->mlflow) (0.47.1)\n",
      "Requirement already satisfied: blinker>=1.9.0 in c:\\jupyter\\venv\\lib\\site-packages (from Flask<4->mlflow) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in c:\\jupyter\\venv\\lib\\site-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in c:\\jupyter\\venv\\lib\\site-packages (from Flask<4->mlflow) (3.1.5)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\jupyter\\venv\\lib\\site-packages (from Flask<4->mlflow) (3.0.2)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in c:\\jupyter\\venv\\lib\\site-packages (from Flask<4->mlflow) (3.1.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\jupyter\\venv\\lib\\site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\jupyter\\venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\jupyter\\venv\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\jupyter\\venv\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (4.7.2)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in c:\\jupyter\\venv\\lib\\site-packages (from graphene<4->mlflow) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in c:\\jupyter\\venv\\lib\\site-packages (from graphene<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in c:\\jupyter\\venv\\lib\\site-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\jupyter\\venv\\lib\\site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.1->mlflow) (3.23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\jupyter\\venv\\lib\\site-packages (from matplotlib<4->mlflow) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\jupyter\\venv\\lib\\site-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\jupyter\\venv\\lib\\site-packages (from matplotlib<4->mlflow) (4.55.6)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\jupyter\\venv\\lib\\site-packages (from matplotlib<4->mlflow) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\jupyter\\venv\\lib\\site-packages (from matplotlib<4->mlflow) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\jupyter\\venv\\lib\\site-packages (from matplotlib<4->mlflow) (3.2.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in c:\\jupyter\\venv\\lib\\site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow) (0.56b0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\jupyter\\venv\\lib\\site-packages (from pandas<3->mlflow) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\jupyter\\venv\\lib\\site-packages (from pandas<3->mlflow) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\jupyter\\venv\\lib\\site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\jupyter\\venv\\lib\\site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\jupyter\\venv\\lib\\site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\jupyter\\venv\\lib\\site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\jupyter\\venv\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\jupyter\\venv\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\jupyter\\venv\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (2024.12.14)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\jupyter\\venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.6.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\jupyter\\venv\\lib\\site-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\jupyter\\venv\\lib\\site-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\jupyter\\venv\\lib\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\jupyter\\venv\\lib\\site-packages (from starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (4.8.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\jupyter\\venv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\jupyter\\venv\\lib\\site-packages (from uvicorn<1->mlflow-skinny==3.1.1->mlflow) (0.14.0)\n",
      "Requirement already satisfied: botocore<1.40.0,>=1.39.4 in c:\\jupyter\\venv\\lib\\site-packages (from boto3) (1.39.4)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\jupyter\\venv\\lib\\site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in c:\\jupyter\\venv\\lib\\site-packages (from boto3) (0.13.0)\n",
      "Requirement already satisfied: docutils<=0.19,>=0.18.1 in c:\\jupyter\\venv\\lib\\site-packages (from awscli) (0.19)\n",
      "Requirement already satisfied: colorlog in c:\\jupyter\\venv\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: tqdm in c:\\jupyter\\venv\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\jupyter\\venv\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow boto3 awscli optuna imbalanced-learn lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e260e49-0137-4565-991a-224ba4a2d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23211106-5e30-4dcf-9197-ca3e0a9b83eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/16 02:28:12 INFO mlflow.tracking.fluent: Experiment with name 'LightGBM HP Tuning' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://mlflow-bucket-1218/397175165646068793', creation_time=1752611294366, experiment_id='397175165646068793', last_update_time=1752611294366, lifecycle_stage='active', name='LightGBM HP Tuning', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set or create an experiment\n",
    "mlflow.set_experiment(\"LightGBM HP Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "963f6927-3837-40ea-986b-60aa5dd81c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\jupyter\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe794393-58f1-465f-993e-3101b5e3bb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36662, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('reddit_preprocessing.csv').dropna(subset=['clean_comment'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eff570c8-a0a1-4c1d-9a75-37c10f7f4f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remap the class labels from [-1, 0, 1] to [2, 0, 1]\n",
    "df['category'] = df['category'].map({-1: 2, 0: 0, 1: 1})\n",
    "\n",
    "# Step 2: Remove rows where the target labels (category) are NaN\n",
    "df = df.dropna(subset=['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0119dfe4-8d60-4f08-b6ae-10771b3f89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: TF-IDF vectorization setup\n",
    "ngram_range = (1, 3)  # Trigram \n",
    "max_features = 10000  # Set max_features to 1000 for TF-IDF\n",
    "vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
    "X = vectorizer.fit_transform(df['clean_comment'])\n",
    "y = df['category']\n",
    "\n",
    "# Step 4: Apply SMOTE to handle class imbalance \n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3ec35dd-dbeb-481a-b42c-f7686b7b8c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train-test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_resampled  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2967277-b53a-4f08-9bf3-e95f81dacb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Function to log results in MLflow with optional params and trial number\n",
    "def log_mlflow(model_name, model, X_train, X_test, y_train, y_test, params=None, trial_num=None):\n",
    "    with mlflow.start_run():\n",
    "        # Log model type and run name with trial number (if provided)\n",
    "        run_name = f\"{model_name}_SMOTE_TFIDF_Trigrams\"\n",
    "        if trial_num is not None:\n",
    "            run_name += f\"_Trial_{trial_num}\"\n",
    "        mlflow.set_tag(\"mlflow.runName\", run_name)\n",
    "        mlflow.set_tag(\"experiment_type\", \"algorithm_comparison\")\n",
    "\n",
    "        # Log algorithm name\n",
    "        mlflow.log_param(\"algo_name\", model_name)\n",
    "\n",
    "        # Log hyperparameters (if any)\n",
    "        if params:\n",
    "            for key, value in params.items():\n",
    "                mlflow.log_param(key, value)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Log accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "        # Log classification report\n",
    "        classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
    "        for label, metrics in classification_rep.items():\n",
    "            if isinstance(metrics, dict):\n",
    "                for metric, value in metrics.items():\n",
    "                    mlflow.log_metric(f\"{label}_{metric}\", value)\n",
    "\n",
    "        # Log the model\n",
    "        mlflow.sklearn.log_model(model, f\"{model_name}_model\")\n",
    "\n",
    "        return accuracy  # Optuna needs this return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62e3cf03-d747-449d-b49a-cd6509e1aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Optuna objective function for LightGBM\n",
    "def objective_lightgbm(trial):\n",
    "    # Hyperparameter space to explore\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "    num_leaves = trial.suggest_int('num_leaves', 20, 150)\n",
    "    min_child_samples = trial.suggest_int('min_child_samples', 10, 100)\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "    subsample = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "    reg_alpha = trial.suggest_float('reg_alpha', 1e-4, 10.0, log=True)\n",
    "    reg_lambda = trial.suggest_float('reg_lambda', 1e-4, 10.0, log=True)\n",
    "\n",
    "    # Log trial parameters\n",
    "    params = {\n",
    "        'n_estimators': n_estimators, \n",
    "        'learning_rate': learning_rate, \n",
    "        'max_depth': max_depth, \n",
    "        'num_leaves': num_leaves,\n",
    "        'min_child_samples':  min_child_samples,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'subsample': subsample,\n",
    "        'reg_alpha': reg_alpha,\n",
    "        'reg_lambda': reg_lambda\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Create LightGBM model\n",
    "    model = XGBClassifier(n_estimators=n_estimators,\n",
    "                          learning_rate=learning_rate,\n",
    "                          max_depth=max_depth,\n",
    "                          num_leaves=num_leaves,\n",
    "                          min_child_samples=min_child_samples,\n",
    "                          colsample_bytree=colsample_bytree,\n",
    "                          subsample=subsample,\n",
    "                          reg_alpha=reg_alpha,\n",
    "                          reg_lambda=reg_lambda,\n",
    "                          random_state=42)\n",
    "\n",
    "    # Log each trial as a separate run in MLflow \n",
    "    accuracy = log_mlflow(\"LightGBM\", model, X_train, X_test, y_train, y_test, params, trial.number)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4d68d1c-1c2f-4c5d-b219-546741c9328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna for LightGBM log the best model, and plot the importance of each parameter \n",
    "def run_optuna_experiment():\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective_lightgbm, n_trials=100)     # Increased to 100 trials\n",
    "\n",
    "    # Get the best parameters\n",
    "    best_params = study.best_params\n",
    "    best_model = LGBMClassifier(n_estimators=best_params['n_estimators'],\n",
    "                          learning_rate=best_params['learning_rate'],\n",
    "                          max_depth=best_params['max_depth'],\n",
    "                          num_leaves=best_params['num_leaves'],\n",
    "                          min_child_samples=best_params['min_child_samples'],\n",
    "                          colsample_bytree=best_params['colsample_bytree'],\n",
    "                          subsample=best_params['subsample'],\n",
    "                          reg_alpha=best_params['reg_alpha'],\n",
    "                          reg_lambda=best_params['reg_lambda'],\n",
    "                          random_state=42)\n",
    "    # Log the best model with MLflow and print the classification report\n",
    "    log_mlflow(\"LightGBM\", best_model, X_train, X_test, y_train, y_test, best_params, \"Best\")\n",
    "\n",
    "    # plot parameter importance\n",
    "    optuna.visualization.plot_param_importances(study).show()\n",
    "\n",
    "    # plot optimization history\n",
    "    optuna.visualization.plot_optimization_history(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f4a1877-4eaa-4a8e-8f6f-ca15b650772b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 02:40:13,526] A new study created in memory with name: no-name-776aa162-2c34-478b-b1de-d47fe9acd9db\n",
      "C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [02:40:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "2025/07/16 02:54:25 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/16 02:54:52 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LightGBM_SMOTE_TFIDF_Trigrams_Trial_0 at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793/runs/4cd22ca6f9b84587a13a3f2d9912829b\n",
      "🧪 View experiment at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 02:55:17,299] Trial 0 finished with value: 0.6553582752060875 and parameters: {'n_estimators': 637, 'learning_rate': 0.0015707911081195751, 'max_depth': 7, 'num_leaves': 147, 'min_child_samples': 40, 'colsample_bytree': 0.7079028253777275, 'subsample': 0.8338870414219501, 'reg_alpha': 0.00017450964997034487, 'reg_lambda': 0.02149601512648243}. Best is trial 0 with value: 0.6553582752060875.\n",
      "C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [02:55:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "2025/07/16 03:17:01 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/16 03:17:25 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LightGBM_SMOTE_TFIDF_Trigrams_Trial_1 at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793/runs/4648d566071e4a028c3108e108fcd9ab\n",
      "🧪 View experiment at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 03:17:36,952] Trial 1 finished with value: 0.7866201648700063 and parameters: {'n_estimators': 647, 'learning_rate': 0.015367117213184015, 'max_depth': 10, 'num_leaves': 31, 'min_child_samples': 37, 'colsample_bytree': 0.7905695694448064, 'subsample': 0.8058104922943897, 'reg_alpha': 0.005902815280261613, 'reg_lambda': 5.4236355288482025}. Best is trial 1 with value: 0.7866201648700063.\n",
      "C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [03:17:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "2025/07/16 03:21:28 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/16 03:21:48 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LightGBM_SMOTE_TFIDF_Trigrams_Trial_2 at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793/runs/bc3ba07b753349268ee79cb09d38b2e0\n",
      "🧪 View experiment at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 03:21:57,711] Trial 2 finished with value: 0.681991122384274 and parameters: {'n_estimators': 720, 'learning_rate': 0.00873969749236792, 'max_depth': 3, 'num_leaves': 69, 'min_child_samples': 61, 'colsample_bytree': 0.500612640113976, 'subsample': 0.7000738648723144, 'reg_alpha': 0.3100391450597955, 'reg_lambda': 0.016844373724445954}. Best is trial 1 with value: 0.7866201648700063.\n",
      "C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [03:22:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "2025/07/16 03:27:16 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/16 03:27:37 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LightGBM_SMOTE_TFIDF_Trigrams_Trial_3 at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793/runs/9e1c6aee1b5b4696a1b6d72a79209346\n",
      "🧪 View experiment at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 03:27:47,973] Trial 3 finished with value: 0.5886704713591207 and parameters: {'n_estimators': 228, 'learning_rate': 0.00040530304115017195, 'max_depth': 6, 'num_leaves': 124, 'min_child_samples': 45, 'colsample_bytree': 0.9819439732413878, 'subsample': 0.6490514176361359, 'reg_alpha': 0.04793111379186892, 'reg_lambda': 0.48135123970637156}. Best is trial 1 with value: 0.7866201648700063.\n",
      "C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [03:27:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "2025/07/16 03:42:56 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/16 03:43:16 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LightGBM_SMOTE_TFIDF_Trigrams_Trial_4 at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793/runs/0f2f177bb5ee4bfd8d90fdc4647d0013\n",
      "🧪 View experiment at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 03:43:26,969] Trial 4 finished with value: 0.6667723525681674 and parameters: {'n_estimators': 664, 'learning_rate': 0.0011444822534894261, 'max_depth': 8, 'num_leaves': 96, 'min_child_samples': 64, 'colsample_bytree': 0.6104002040163097, 'subsample': 0.5366884952097877, 'reg_alpha': 0.09388310092245415, 'reg_lambda': 0.00014932618026299087}. Best is trial 1 with value: 0.7866201648700063.\n",
      "C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [03:43:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "2025/07/16 03:49:09 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/16 03:49:29 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LightGBM_SMOTE_TFIDF_Trigrams_Trial_5 at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793/runs/7211ee60c1164204873a186e6a1ec9ed\n",
      "🧪 View experiment at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 03:49:38,547] Trial 5 finished with value: 0.6576833650391037 and parameters: {'n_estimators': 360, 'learning_rate': 0.0032130651669265873, 'max_depth': 6, 'num_leaves': 24, 'min_child_samples': 55, 'colsample_bytree': 0.5822965384997418, 'subsample': 0.56731291897746, 'reg_alpha': 0.9051330973992038, 'reg_lambda': 0.1492989422122271}. Best is trial 1 with value: 0.7866201648700063.\n",
      "C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [03:49:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "2025/07/16 04:01:15 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/16 04:01:35 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LightGBM_SMOTE_TFIDF_Trigrams_Trial_6 at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793/runs/ec74a289e0784c68ac250b053891cd50\n",
      "🧪 View experiment at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 04:01:44,251] Trial 6 finished with value: 0.5900443880786304 and parameters: {'n_estimators': 787, 'learning_rate': 0.00042248533477644843, 'max_depth': 5, 'num_leaves': 141, 'min_child_samples': 41, 'colsample_bytree': 0.85106107577183, 'subsample': 0.8730011921237606, 'reg_alpha': 0.09210480587829592, 'reg_lambda': 0.02052990954045522}. Best is trial 1 with value: 0.7866201648700063.\n",
      "C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [04:01:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "2025/07/16 04:09:34 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/16 04:09:54 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LightGBM_SMOTE_TFIDF_Trigrams_Trial_7 at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793/runs/93f941eeefbb427c846f290c3bc279b1\n",
      "🧪 View experiment at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 04:10:03,502] Trial 7 finished with value: 0.6455294863665187 and parameters: {'n_estimators': 349, 'learning_rate': 0.0001348086651046975, 'max_depth': 7, 'num_leaves': 146, 'min_child_samples': 74, 'colsample_bytree': 0.614860559808722, 'subsample': 0.9649266503167551, 'reg_alpha': 0.023773035512646465, 'reg_lambda': 2.696644748548863}. Best is trial 1 with value: 0.7866201648700063.\n",
      "C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [04:10:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "2025/07/16 04:13:12 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/16 04:13:31 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LightGBM_SMOTE_TFIDF_Trigrams_Trial_8 at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793/runs/c94678deaae94207b8173217e3892215\n",
      "🧪 View experiment at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 04:13:48,564] Trial 8 finished with value: 0.8044810822236313 and parameters: {'n_estimators': 601, 'learning_rate': 0.07087938820058017, 'max_depth': 3, 'num_leaves': 113, 'min_child_samples': 58, 'colsample_bytree': 0.5170591343144709, 'subsample': 0.9556230728957809, 'reg_alpha': 0.007935911850577985, 'reg_lambda': 0.10337658763480631}. Best is trial 8 with value: 0.8044810822236313.\n",
      "C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [04:13:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "2025/07/16 04:24:20 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/16 04:24:40 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LightGBM_SMOTE_TFIDF_Trigrams_Trial_9 at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793/runs/e04cf324343e4e2c88f58601ed1d37d9\n",
      "🧪 View experiment at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 04:24:49,343] Trial 9 finished with value: 0.611709997886282 and parameters: {'n_estimators': 888, 'learning_rate': 0.000351138960300089, 'max_depth': 5, 'num_leaves': 67, 'min_child_samples': 90, 'colsample_bytree': 0.609336333732238, 'subsample': 0.8728637009894099, 'reg_alpha': 0.004541123749709306, 'reg_lambda': 0.014169098586292271}. Best is trial 8 with value: 0.8044810822236313.\n",
      "C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [04:24:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "2025/07/16 04:32:33 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/16 04:32:53 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LightGBM_SMOTE_TFIDF_Trigrams_Trial_10 at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793/runs/32cba40c68e74648b29a51a4d34abd6d\n",
      "🧪 View experiment at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 04:33:02,616] Trial 10 finished with value: 0.8517226801944621 and parameters: {'n_estimators': 995, 'learning_rate': 0.07537964931685281, 'max_depth': 4, 'num_leaves': 105, 'min_child_samples': 10, 'colsample_bytree': 0.7181392552322692, 'subsample': 0.9988564119670158, 'reg_alpha': 0.00027809363888312396, 'reg_lambda': 0.0010935984317003548}. Best is trial 10 with value: 0.8517226801944621.\n",
      "C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [04:33:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "2025/07/16 04:38:08 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/16 04:38:28 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LightGBM_SMOTE_TFIDF_Trigrams_Trial_11 at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793/runs/56f6374a51c5414aa5950659faefa739\n",
      "🧪 View experiment at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 04:38:45,893] Trial 11 finished with value: 0.8451701543014162 and parameters: {'n_estimators': 928, 'learning_rate': 0.0944307098598263, 'max_depth': 3, 'num_leaves': 105, 'min_child_samples': 13, 'colsample_bytree': 0.7177024375705771, 'subsample': 0.9521965749978787, 'reg_alpha': 0.00011634788251852515, 'reg_lambda': 0.001281398705560109}. Best is trial 10 with value: 0.8517226801944621.\n",
      "C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [04:38:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "2025/07/16 04:46:20 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/16 04:46:39 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LightGBM_SMOTE_TFIDF_Trigrams_Trial_12 at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793/runs/904930a1a35d45429507e175320c652e\n",
      "🧪 View experiment at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 04:46:48,737] Trial 12 finished with value: 0.8618685267385331 and parameters: {'n_estimators': 978, 'learning_rate': 0.09590521242165773, 'max_depth': 4, 'num_leaves': 93, 'min_child_samples': 13, 'colsample_bytree': 0.7211175197375912, 'subsample': 0.9976688764571017, 'reg_alpha': 0.0001575334480175607, 'reg_lambda': 0.00046946829882208406}. Best is trial 12 with value: 0.8618685267385331.\n",
      "C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [04:46:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "2025/07/16 04:54:26 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/16 04:54:46 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LightGBM_SMOTE_TFIDF_Trigrams_Trial_13 at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793/runs/f0f7f47d1aaa45acba979741f2900a83\n",
      "🧪 View experiment at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 04:54:55,590] Trial 13 finished with value: 0.7950750369900655 and parameters: {'n_estimators': 984, 'learning_rate': 0.02676215681700023, 'max_depth': 4, 'num_leaves': 78, 'min_child_samples': 11, 'colsample_bytree': 0.8096092276630465, 'subsample': 0.9918113606789309, 'reg_alpha': 0.000869521092259799, 'reg_lambda': 0.0007008976283315045}. Best is trial 12 with value: 0.8618685267385331.\n",
      "C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [04:55:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LightGBM_SMOTE_TFIDF_Trigrams_Trial_14 at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793/runs/b5c75e3ab0d84f119fdb4c9d07752f78\n",
      "🧪 View experiment at: http://ec2-18-215-180-118.compute-1.amazonaws.com:5000/#/experiments/397175165646068793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-07-16 05:01:39,896] Trial 14 failed with parameters: {'n_estimators': 835, 'learning_rate': 0.028138210127115152, 'max_depth': 4, 'num_leaves': 49, 'min_child_samples': 24, 'colsample_bytree': 0.9049595184375487, 'subsample': 0.9044445499671364, 'reg_alpha': 7.714336431141125, 'reg_lambda': 0.0020851515066384447} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\jupyter\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\latif\\AppData\\Local\\Temp\\ipykernel_13356\\3259828239.py\", line 41, in objective_lightgbm\n",
      "    accuracy = log_mlflow(\"LightGBM\", model, X_train, X_test, y_train, y_test, params, trial.number)\n",
      "  File \"C:\\Users\\latif\\AppData\\Local\\Temp\\ipykernel_13356\\3670938415.py\", line 20, in log_mlflow\n",
      "    model.fit(X_train, y_train)\n",
      "    ~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1682, in fit\n",
      "    self._Booster = train(\n",
      "                    ~~~~~^\n",
      "        params,\n",
      "        ^^^^^^^\n",
      "    ...<9 lines>...\n",
      "        callbacks=self.callbacks,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\training.py\", line 183, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\core.py\", line 2247, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self.handle, ctypes.c_int(iteration), dtrain.handle\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "KeyboardInterrupt\n",
      "[W 2025-07-16 05:01:39,986] Trial 14 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run the experiment for LightGBM\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrun_optuna_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36mrun_optuna_experiment\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_optuna_experiment\u001b[39m():\n\u001b[0;32m      3\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_lightgbm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m     \u001b[38;5;66;03m# Increased to 100 trials\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Get the best parameters\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[1;32mC:\\jupyter\\venv\\Lib\\site-packages\\optuna\\study\\study.py:489\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    389\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\jupyter\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 64\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mC:\\jupyter\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mC:\\jupyter\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    249\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    252\u001b[0m ):\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mC:\\jupyter\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[18], line 41\u001b[0m, in \u001b[0;36mobjective_lightgbm\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBClassifier(n_estimators\u001b[38;5;241m=\u001b[39mn_estimators,\n\u001b[0;32m     30\u001b[0m                       learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[0;32m     31\u001b[0m                       max_depth\u001b[38;5;241m=\u001b[39mmax_depth,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m                       reg_lambda\u001b[38;5;241m=\u001b[39mreg_lambda,\n\u001b[0;32m     38\u001b[0m                       random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Log each trial as a separate run in MLflow \u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mlog_mlflow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLightGBM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "Cell \u001b[1;32mIn[17], line 20\u001b[0m, in \u001b[0;36mlog_mlflow\u001b[1;34m(model_name, model, X_train, X_test, y_train, y_test, params, trial_num)\u001b[0m\n\u001b[0;32m     17\u001b[0m         mlflow\u001b[38;5;241m.\u001b[39mlog_param(key, value)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Log accuracy\u001b[39;00m\n",
      "File \u001b[1;32mC:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\sklearn.py:1682\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1660\u001b[0m model, metric, params, feature_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(\n\u001b[0;32m   1661\u001b[0m     xgb_model, params, feature_weights\n\u001b[0;32m   1662\u001b[0m )\n\u001b[0;32m   1663\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1664\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1665\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1680\u001b[0m )\n\u001b[1;32m-> 1682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mC:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\training.py:183\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\jupyter\\venv\\Lib\\site-packages\\xgboost\\core.py:2247\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2246\u001b[0m     _check_call(\n\u001b[1;32m-> 2247\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2250\u001b[0m     )\n\u001b[0;32m   2251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2252\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the experiment for LightGBM\n",
    "run_optuna_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c45f6-38b6-4767-9b3c-f3c3cfd48ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
